# ðŸ“š Complete Module Documentation

## Table of Contents

1. [Module 1: Document Preprocessor](#module-1-document-preprocessor)
2. [Module 2: Multi-Engine OCR](#module-2-multi-engine-ocr)
3. [Module 3: Question Paper Processor](#module-3-question-paper-processor)
4. [Module 4: Answer Key Processor](#module-4-answer-key-processor)
5. [Module 5: Student Answer Processor](#module-5-student-answer-processor)

6. [Module 6: Evaluation Engines](#module-6-evaluation-engines)
7. [Module 7: Confidence Scorer](#module-7-confidence-scorer)
8. [Module 10: Parallel Processing](#module-10-parallel-processing)
9. [Enhanced Features](#enhanced-features)

---

## Module 1: Document Preprocessor

**File**: `backend/app/services/preprocessor/pdf_processor.py`

### Purpose
Converts PDFs to high-quality images and enhances them for optimal OCR performance.

### Key Classes

#### `DocumentPreprocessor`

**Initialization:**
```python
from app.services.preprocessor.pdf_processor import DocumentPreprocessor

preprocessor = DocumentPreprocessor(dpi=300)
```

**Methods:**

##### `pdf_to_images(pdf_path: str) -> List[np.ndarray]`
Converts PDF pages to images.

```python
images = preprocessor.pdf_to_images("question_paper.pdf")
# Returns: List of numpy arrays (one per page)
```

##### `enhance_image(image: np.ndarray) -> np.ndarray`
Applies enhancement techniques:
- Converts to grayscale
- Denoises using fastNlMeansDenoising
- Deskews (corrects rotation)
- Enhances contrast with CLAHE

```python
enhanced = preprocessor.enhance_image(image)
```

##### `binarize_image(image: np.ndarray) -> np.ndarray`
Converts to binary (black & white) for optimal OCR.

```python
binary = preprocessor.binarize_image(enhanced)
```

##### `preprocess_document(pdf_path: str) -> List[Tuple]`
Complete pipeline: PDF â†’ Enhanced â†’ Binary

```python
processed = preprocessor.preprocess_document("exam.pdf")
# Returns: [(enhanced_img, binary_img), ...] for each page
```

##### `extract_region(image, top_percent, bottom_percent, left_percent, right_percent)`
Extracts specific region (useful for headers).

```python
header = preprocessor.extract_region(
    image,
    top_percent=0,
    bottom_percent=15  # Top 15% of page
)
```

### How It Works

1. **PDF â†’ Images**: Uses `pdf2image` at 300 DPI
2. **Enhancement**:
   - Grayscale conversion
   - Noise removal
   - Rotation correction (deskewing)
   - Contrast enhancement
3. **Binarization**: Adaptive thresholding for varying lighting
4. **Output**: Clean, OCR-ready images

---

## Module 2: Multi-Engine OCR

**File**: `backend/app/services/ocr/ocr_engine.py`

### Purpose
Extracts text from images using multiple OCR engines based on content type.

### Key Classes

#### `OCRResult`
Container for OCR results.

```python
class OCRResult:
    text: str              # Extracted text
    confidence: float      # Overall confidence (0.0-1.0)
    word_confidences: List[float]  # Per-word confidence
```

#### `HybridOCR`
Main OCR class that routes to appropriate engine.

**Initialization:**
```python
from app.services.ocr.ocr_engine import create_ocr_engine

ocr = create_ocr_engine(
    google_api_key="your_google_key",
    mathpix_app_id="your_mathpix_id",
    mathpix_app_key="your_mathpix_key"
)
```

**Methods:**

##### `detect_content_type(image) -> str`
Detects content type: 'text', 'math', or 'mixed'

```python
content_type = ocr.detect_content_type(image)
# Returns: 'text', 'math', or 'mixed'
```

##### `extract_text_with_confidence(image, content_type=None) -> OCRResult`
Extracts text using appropriate engine.

```python
result = ocr.extract_text_with_confidence(image)
print(result.text)
print(f"Confidence: {result.confidence}")
```

##### `extract_math_equations(image) -> List[str]`
Extracts only math equations.

```python
equations = ocr.extract_math_equations(image)
# Returns: ['\\int x^2 dx', '\\frac{a}{b}', ...]
```

### How It Works

1. **Content Detection**: Analyzes text for math indicators
2. **Engine Selection**:
   - **Text**: Google Cloud Vision (handwriting)
   - **Math**: Mathpix (equations, formulas)
   - **Mixed**: Both engines
3. **Fallback**: Mock OCR if APIs not configured

### Supported Content

- âœ… Handwritten text
- âœ… Printed text
- âœ… Mathematical equations
- âœ… Chemical formulas (via Mathpix)
- âœ… Mixed content

---

## Module 3: Question Paper Processor

**File**: `backend/app/services/processors/question_paper_processor.py`

### Purpose
Extracts questions, marks allocation, and Bloom's taxonomy from question papers.

### Key Classes

#### `QuestionPaperProcessor`

**Initialization:**
```python
from app.services.processors.question_paper_processor import QuestionPaperProcessor

processor = QuestionPaperProcessor(ocr_engine)
```

**Methods:**

##### `extract_questions(ocr_text: str) -> List[Dict]`
Extracts all questions from OCR text.

```python
questions = processor.extract_questions(full_text)
# Returns: [
#   {
#     "number": "1",
#     "text": "Explain photosynthesis...",
#     "max_marks": 10,
#     "bloom_level": "Understand",
#     "question_type": "text"
#   },
#   ...
# ]
```

##### `extract_marks(question_text: str) -> int`
Extracts marks from question text.

Recognizes patterns:
- `[5 marks]`
- `(10 marks)`
- `Marks: 5`

```python
marks = processor.extract_marks("Explain... [10 marks]")
# Returns: 10
```

##### `extract_bloom_level(question_text: str) -> str`
Identifies Bloom's taxonomy level.

Levels:
- Remember (define, list, name)
- Understand (explain, describe)
- Apply (calculate, solve)
- Analyze (examine, differentiate)
- Evaluate (assess, critique)
- Create (design, develop)

```python
level = processor.extract_bloom_level("Explain the process...")
# Returns: "Understand"
```

##### `process_question_paper(images, question_paper_id) -> List[Dict]`
Complete processing pipeline.

```python
questions = processor.process_question_paper(
    images=preprocessed_images,
    question_paper_id=1
)
```

### Question Number Patterns

Recognizes:
- `Q1:`, `Q.1)`, `Q1.`
- `Question 1:`
- `1)`, `1.`
- `Q1(a)`, `Q1a`

---

## Module 4: Answer Key Processor

**File**: `backend/app/services/processors/answer_key_processor.py`

### Purpose
Extracts model answers and generates keywords/marking schemes using AI.

### Key Classes

#### `AnswerKeyProcessor`

**Initialization:**
```python
from app.services.processors.answer_key_processor import AnswerKeyProcessor

processor = AnswerKeyProcessor(ocr_engine, ai_evaluator)
```

**Methods:**

##### `extract_model_answers(ocr_text, questions) -> List[Dict]`
Matches answers to questions.

```python
model_answers = processor.extract_model_answers(full_text, questions)
```

##### `extract_keywords_with_ai(model_answer_text, question_text) -> List[str]`
Uses AI to extract key concepts.

```python
keywords = processor.extract_keywords_with_ai(
    "Photosynthesis is the process...",
    "Explain photosynthesis"
)
# Returns: ['photosynthesis', 'chlorophyll', 'light energy', 'glucose', ...]
```

##### `create_marking_scheme(model_answer, keywords, max_marks) -> Dict`
Generates AI-powered marking scheme.

```python
scheme = processor.create_marking_scheme(
    model_answer_text="...",
    keywords=['concept1', 'concept2'],
    max_marks=10
)
# Returns: {
#   "full_marks_criteria": "Complete answer covering all concepts",
#   "partial_credit": [
#     {"marks": 7, "criteria": "Covers most key concepts"},
#     {"marks": 5, "criteria": "Covers some key concepts"}
#   ],
#   "deductions": ["Missing key concepts", "Factual errors"]
# }
```

##### `process_answer_key(images, questions, answer_key_id) -> List[Dict]`
Complete processing with AI keyword extraction.

```python
model_answers = processor.process_answer_key(
    images=preprocessed_images,
    questions=questions,
    answer_key_id=1
)
```

---

## Module 5: Student Answer Processor

**File**: `backend/app/services/processors/student_answer_processor.py`

### Purpose
Processes student answer sheets with smart question mapping.

### Key Classes

#### `StudentAnswerSheetProcessor`

**Initialization:**
```python
from app.services.processors.student_answer_processor import StudentAnswerSheetProcessor

processor = StudentAnswerSheetProcessor(ocr_engine, preprocessor)
```

**Methods:**

##### `extract_header_info(first_page_image) -> Dict`
Extracts student information from header.

```python
info = processor.extract_header_info(first_page)
# Returns: {
#   "roll_number": "CS2021001",
#   "name": "John Doe",
#   "course_code": "CS101",
#   "date": "15/01/2024"
# }
```

##### `scan_all_question_numbers(full_text) -> List[Tuple]`
**CRITICAL**: Scans entire document for ALL question numbers.

```python
positions = processor.scan_all_question_numbers(full_text)
# Returns: [("1", 150), ("5", 800), ("3", 1200), ("2", 1500)]
# (question_number, position_in_text)
```

##### `map_answers_to_questions(full_text, questions) -> List[Dict]`
Maps answers to questions (handles out-of-order).

```python
answers = processor.map_answers_to_questions(full_text, questions)
# Returns: [
#   {
#     "question_id": 1,
#     "question_number": "1",
#     "answer_text": "...",
#     "is_attempted": True,
#     "position_in_sheet": 1
#   },
#   ...
# ]
```

**Key Feature**: Handles out-of-order answers!
- Student writes: Q1, Q5, Q3, Q2
- System correctly maps each answer
- Only marks as "not attempted" after scanning full document

##### `process_student_answer_sheet(images, questions, student_exam_id) -> Dict`
Complete processing pipeline.

```python
result = processor.process_student_answer_sheet(
    images=preprocessed_images,
    questions=questions,
    student_exam_id=1
)
# Returns: {
#   "header_info": {...},
#   "answers": [...],
#   "total_questions": 10,
#   "attempted_questions": 8,
#   "unattempted_questions": 2
# }
```

---

## Module 6: Evaluation Engines

### 6A: Text Answer Evaluator

**File**: `backend/app/services/evaluators/text_evaluator.py`

**Purpose**: Evaluates theory/descriptive answers.

**Usage:**
```python
from app.services.evaluators.text_evaluator import evaluate_text

result = evaluate_text(
    student_answer="Photosynthesis is...",
    model_answer="Photosynthesis is the process...",
    keywords=['photosynthesis', 'chlorophyll', 'glucose'],
    max_marks=10,
    ai_evaluator=ai,
    question_text="Explain photosynthesis"
)

# Returns: {
#   "marks_awarded": 7.5,
#   "max_marks": 10,
#   "feedback": "Good understanding of core concepts...",
#   "strengths": ["Clear explanation", "Correct approach"],
#   "improvements": ["Add more examples"],
#   "keywords_matched": 8,
#   "total_keywords": 10
# }
```

**Evaluation Strategy**:
- Strict on factual accuracy
- Context-aware on wording
- Partial credit for partial concepts
- Keyword coverage analysis

---

### 6B: Math Answer Evaluator

**File**: `backend/app/services/evaluators/math_evaluator.py`

**Purpose**: Evaluates mathematical answers with step-wise grading.

**Usage:**
```python
from app.services.evaluators.math_evaluator import evaluate_math

result = evaluate_math(
    student_answer="Step 1: ...\nStep 2: ...",
    model_answer="Step 1: ...\nStep 2: ...",
    max_marks=10,
    ai_evaluator=ai
)

# Returns: {
#   "marks_awarded": 8.0,
#   "correct_steps": 4,
#   "total_steps": 5,
#   "final_answer_correct": True,
#   "method_score": 9.0,
#   "feedback": "Correct method, minor calculation error",
#   "step_breakdown": "Steps 1-4 correct, step 5 arithmetic error"
# }
```

**Features**:
- Automatic step extraction
- Partial credit for correct method
- Distinguishes method vs calculation errors

---

### 6C: Code Answer Evaluator

**File**: `backend/app/services/evaluators/code_evaluator.py`

**Purpose**: Evaluates programming answers (NO CODE EXECUTION).

**Usage:**
```python
from app.services.evaluators.code_evaluator import evaluate_code

result = evaluate_code(
    student_answer="def factorial(n):\n    if n == 0: return 1...",
    model_answer="def factorial(n):\n    ...",
    max_marks=10,
    ai_evaluator=ai,
    language="python"
)

# Returns: {
#   "marks_awarded": 9.0,
#   "logic_score": 9.0,
#   "approach_correct": "Yes",
#   "feedback": "Excellent algorithm...",
#   "strengths": "Clear logic, efficient",
#   "improvements": "Could optimize loop",
#   "edge_cases": "Handles empty input well"
# }
```

**Evaluation Focus**:
- Algorithm correctness
- Logic flow
- Ignores variable names
- Ignores minor syntax
- **Security**: Never executes code

---

### 6D: Diagram Answer Evaluator

**File**: `backend/app/services/evaluators/diagram_evaluator.py`

**Purpose**: Evaluates diagrams (always flags for review).

**Usage:**
```python
from app.services.evaluators.diagram_evaluator import evaluate_diagram

result = evaluate_diagram(
    ocr_text="Heart\nAorta\nVentricle",
    required_components=['Heart', 'Aorta', 'Ventricle', 'Atrium'],
    max_marks=10
)

# Returns: {
#   "marks_awarded": 7.5,
#   "extracted_labels": ["Heart", "Aorta", "Ventricle"],
#   "matched_components": ["Heart", "Aorta", "Ventricle"],
#   "missing_components": ["Atrium"],
#   "match_percentage": 75.0,
#   "needs_review": True,  # ALWAYS True
#   "review_reason": "Diagram requires visual verification"
# }
```

---

### 6E: MCQ Evaluator

**File**: `backend/app/services/evaluators/mcq_evaluator.py`

**Purpose**: Evaluates multiple choice questions.

**Usage:**
```python
from app.services.evaluators.mcq_evaluator import evaluate_mcq

result = evaluate_mcq(
    student_answer="B",
    correct_option="B",
    marks_per_question=1
)

# Returns: {
#   "marks_awarded": 1,
#   "selected_option": "B",
#   "correct_option": "B",
#   "is_correct": True,
#   "feedback": "âœ“ Correct! Selected option B."
# }
```

**Recognizes Patterns**:
- `A`, `B`, `C`, `D`
- `(A)`, `(B)`
- `A.`, `B.`
- `Option A`, `Answer: B`

---

## Module 7: Confidence Scorer

**File**: `backend/app/services/ocr/confidence_scorer.py`

### Purpose
Determines when answers need manual review.

**Usage:**
```python
from app.services.ocr.confidence_scorer import score_answer_confidence

confidence = score_answer_confidence(
    ocr_result=ocr_result,
    evaluation_result=eval_result,
    answer_type="text",
    is_attempted=True
)

# Returns: {
#   "ocr_confidence": 0.88,
#   "evaluation_confidence": 0.75,
#   "overall_confidence": 0.80,
#   "confidence_level": "High",
#   "needs_review": False,
#   "review_reasons": []
# }
```

**Flagging Rules**:
- OCR confidence < 70% â†’ Flag
- Diagrams â†’ Always flag
- Low evaluation confidence â†’ Flag
- Ambiguous content â†’ Flag

---

## Module 10: Parallel Processing

**File**: `backend/app/services/batch/parallel_processor.py`

### Purpose
Enables concurrent evaluation of multiple courses.

**Usage:**
```python
from app.services.batch.parallel_processor import evaluate_courses_parallel

courses = [
    {"course_code": "CS101"},
    {"course_code": "CS102"},
    {"course_code": "MATH201"},
]

def process_course(course):
    # Your processing logic
    return {"status": "completed"}

results = evaluate_courses_parallel(
    courses=courses,
    evaluation_function=process_course,
    max_workers=4  # 4 parallel workers
)

# Returns: {
#   "results": {...},
#   "errors": {...},
#   "total_courses": 3,
#   "successful": 3,
#   "failed": 0,
#   "elapsed_time": 45.2,
#   "avg_time_per_course": 15.1
# }
```

**Performance**:
- 4 cores: 4x speedup
- 8 cores: 7x speedup

---

## Enhanced Features

### Diagram Analyzer

**File**: `backend/app/services/processors/diagram_analyzer.py`

**Features**:
- Component counting
- Arrow detection
- Visual complexity assessment

```python
from app.services.processors.diagram_analyzer import analyze_diagram

analysis = analyze_diagram(
    image=diagram_image,
    required_components=['Heart', 'Aorta'],
    ocr_engine=ocr
)

# Returns: {
#   "diagram_detected": True,
#   "component_count": 12,
#   "arrow_count": 5,
#   "visual_complexity": "Medium",
#   "match_percentage": 75.0
# }
```

### Model Fine-Tuning

**File**: `backend/app/services/processors/model_training.py`

**Purpose**: Learns faculty marking patterns.

```python
from app.services.processors.model_training import MarkingPatternLearner

learner = MarkingPatternLearner()

# Record faculty adjustment
learner.record_faculty_adjustment(
    course_code="CS101",
    question_type="text",
    ai_marks=7.5,
    faculty_marks=8.0,
    reason="Good understanding"
)

# Get adjusted marks (after learning)
adjusted = learner.get_adjusted_marks("CS101", "text", 7.5, 10)
# Returns: 8.0 (learned to be more lenient)
```

### Feedback Generator

**Purpose**: Comprehensive feedback for every answer.

```python
from app.services.processors.model_training import FeedbackGenerator

generator = FeedbackGenerator()

feedback = generator.generate_answer_feedback(
    question=question_data,
    student_answer=answer_data,
    evaluation_result=eval_result
)

# Returns detailed feedback with strengths, improvements, suggestions
```

---

## Complete Workflow Example

```python
# 1. Initialize services
preprocessor = DocumentPreprocessor(dpi=300)
ocr = create_ocr_engine(gemini_api_key="key")
ai = create_ai_evaluator(gemini_api_key="key")

# 2. Process question paper
qp_images = preprocessor.preprocess_document("qp.pdf")
questions = process_question_paper(qp_images, 1, ocr)

# 3. Process answer key
ak_images = preprocessor.preprocess_document("ak.pdf")
model_answers = process_answer_key(ak_images, questions, 1, ocr, ai)

# 4. Process student sheet
student_images = preprocessor.preprocess_document("student.pdf")
student_result = process_student_sheet(student_images, questions, 1, ocr, preprocessor)

# 5. Evaluate each answer
for answer in student_result['answers']:
    if answer['is_attempted']:
        eval_result = evaluate_text(
            answer['answer_text'],
            model_answers[0]['text'],
            model_answers[0]['keywords'],
            10,
            ai
        )
        print(f"Marks: {eval_result['marks_awarded']}/10")

# 6. Generate feedback
feedback_gen = FeedbackGenerator()
feedback = feedback_gen.generate_answer_feedback(
    questions[0], answer, eval_result
)
```

---

**All modules are production-ready and fully documented!** ðŸ“š
